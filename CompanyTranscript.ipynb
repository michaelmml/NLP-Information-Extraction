{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CompanyNLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4YSWaSCDuFDt0QewTE8AY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Built through **PDF Plumber** and depends on structure of the company's transcript releases - intuitive to adapt code. Consider the data in its raw form, consisting of text and \\n (new lines). Normally, these are consistent for companies and therefore straightforward to implement rules to extract **dates, names,** etc. depending on where they are positioned in the format of the transcript.\n",
        "\n",
        "For example, the dates for the below transcript form follow from 'Earnings Call' in the title. By finding the index of where that title ends: \n",
        "\n",
        "**date_idx = txt.find('EVENT DATE/TIME: ') + len('EVENT DATE/TIME: ')** \n",
        "\n",
        "We can find a range that encompasses the title and use split() to identify the month, day, year, etc."
      ],
      "metadata": {
        "id": "hsn2ODhiMLCq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBh_nfAtKpMX"
      },
      "outputs": [],
      "source": [
        "def transcript_convert(ticker):\n",
        "    path = '...Transcripts/ALK'\n",
        "    # path = os.path.join(*paths, ticker)\n",
        "    pdf_list = [x for x in os.listdir(path) if x[-4:] == '.pdf']\n",
        "    print(pdf_list)\n",
        "    columns = ['particip', 'qna', 'transcript', 'date', 'company']\n",
        "    data = pd.DataFrame(columns=columns)\n",
        "    for f in pdf_list:\n",
        "        found_start = False\n",
        "        out = ''\n",
        "        with pdfplumber.open(os.path.join(path, f)) as pdf:\n",
        "\n",
        "            for i,v in enumerate(pdf.pages):\n",
        "                curr_page = pdf.pages[i]\n",
        "                txt = curr_page.extract_text()\n",
        "\n",
        "                if i == 0:\n",
        "                    if txt.find(ticker + ' -') != -1:\n",
        "                        dt_idx = txt.find(ticker + ' -')\n",
        "                        qt_txt = txt[dt_idx:].split('\\n')\n",
        "                        qt_txt = [x for x in qt_txt if x != ' '][0].strip(ticker + ' -')  # remove ' '\n",
        "                    else:\n",
        "                        qt_txt = [x for x in txt.split('\\n') if 'Earnings' in x][0]\n",
        "                    quarter = qt_txt.split()[0]\n",
        "                    year = qt_txt.split()[1]\n",
        "                    date_idx = txt.find('EVENT DATE/TIME: ') + len('EVENT DATE/TIME: ')\n",
        "                    date_idx_end = date_idx + 50  # static character count for length of date\n",
        "                    date = parse(txt[date_idx:date_idx_end].split('\\n')[0]).strftime('%m-%d-%y')\n",
        "                    comp_name = qt_txt.split(' Earnings Call')[0][qt_txt.split(' Earnings Call')[0].find(year)+ len(year) + 1:]\n",
        "\n",
        "                elif i == 1:\n",
        "                    corp_idx = txt.find('CORPORATE PARTICIPANTS')+len('CORPORATE PARTICIPANTS\\n')\n",
        "                    corp_idx_end = txt.find('\\nCONFERENCE CALL PARTICIPANTS')\n",
        "                    conf_idx = txt.find('CONFERENCE CALL PARTICIPANTS') + len('CONFERENCE CALL PARTICIPANTS\\n')\n",
        "                    conf_idx_end = txt.find('PRESENTATION')\n",
        "                    corp_txt = txt[corp_idx:corp_idx_end].split('\\n')\n",
        "                    corp_name = [x.split(comp_name.split()[0])[0] for x in corp_txt]\n",
        "                    corp_name = [x.strip() for x in corp_name if x.strip() != '']\n",
        "                    corp_speaker1 = corp_name\n",
        "                    corp_speaker2 = [x.split()[0] + ' ' + x.split()[1] for x in corp_txt if len(x.split()) >= 2]\n",
        "                    corp_speaker3 = [x.split()[0] + ' ' + x.split()[2] for x in corp_txt if\n",
        "                                     len(x.split()) >= 3]\n",
        "                    corp_speaker4 = [x.split()[0] + ' ' + x.split()[1] + ' ' + x.split()[2] for x in corp_txt if\n",
        "                                     len(x.split()) >= 3]\n",
        "\n",
        "                    conf_txt = txt[conf_idx:conf_idx_end].split('\\n')\n",
        "                    conf_speaker1 = [x.split()[0] + ' ' + x.split()[1] for x in conf_txt if len(x.split()) >= 2]\n",
        "                    conf_speaker2 = [x.split()[0] + ' ' + x.split()[2] for x in conf_txt if len(x.split()) >= 3]    # accounts for speakers with middle name\n",
        "                    conf_speaker3 = [x.split()[0] + ' ' + x.split()[1] + ' ' + x.split()[2] for x in conf_txt if len(x.split()) >=3]    # accounts for speakers with middle name\n",
        "\n",
        "                    speaker_list = corp_speaker1 + corp_speaker2 + corp_speaker3 + corp_speaker4 + conf_speaker1 + conf_speaker2 + conf_speaker3\n",
        "\n",
        "                    columns = ['corp_particip','conf_particip']\n",
        "                    df_speaker = pd.DataFrame(columns=columns)\n",
        "                    corp_list = corp_speaker1 + corp_speaker2 + corp_speaker3 + corp_speaker4\n",
        "                    conf_list = conf_speaker1 + conf_speaker2 + conf_speaker3\n",
        "                    df_speaker['corp_particip'] = corp_list + (max(len(corp_list), len(conf_list)) -len(corp_list)) * ['']\n",
        "                    df_speaker['conf_particip'] = conf_list + (max(len(corp_list), len(conf_list)) -len(conf_list)) * ['']\n",
        "                    df_speaker.to_csv(os.path.join(path, ticker + '_' + date + '_speakers.csv'), index=False)\n",
        "\n",
        "                # remove footer\n",
        "                if i != 0 and i != len(pdf.pages)-1:\n",
        "                    beginning_idx = txt.find('Earnings Call')+len('Earnings Call\\n')\n",
        "                    pg_num_idx = txt.find('\\n' + str(i + 1))\n",
        "                    txt = txt[beginning_idx:pg_num_idx]\n",
        "\n",
        "                # remove disclaimer\n",
        "                elif i ==len(pdf.pages)-1:  # if last page remove disclaimer\n",
        "                    beginning_idx = txt.find('Earnings Call')+len('Earnings Call\\n')\n",
        "                    dis_idx = txt.find('DISCLAIMER')\n",
        "                    txt = txt[beginning_idx:dis_idx]\n",
        "\n",
        "                # write to file\n",
        "                if found_start == True:\n",
        "                    out = out+txt\n",
        "\n",
        "                elif txt.find('PRESENTATION') != -1 and found_start == False:\n",
        "                    found_start = True\n",
        "                    start_idx = txt.find('PRESENTATION')+len('PRESENTATION\\n')\n",
        "                    out = out+txt[start_idx:]\n",
        "\n",
        "        columns = ['particip', 'qna', 'transcript', 'date', 'company']\n",
        "        df = pd.DataFrame(columns=columns)\n",
        "        speaker = ''\n",
        "        content = ''\n",
        "        qna_start_j = 1000  # initialize\n",
        "        j = 0\n",
        "        for line in out.split('\\n'):\n",
        "\n",
        "            if line.find('QUESTIONS AND ANSWERS') != -1:\n",
        "                qna_start_j = j + 2\n",
        "            if len(line.split()) == 1:\n",
        "                find_speaker = [x for x in ['Operator'] if (x in line)]\n",
        "            else:\n",
        "                find_speaker = [x for x in speaker_list if (x + ' -' in line)]  # + ' -'\n",
        "\n",
        "            if find_speaker == []:\n",
        "                content = content + ' ' + line\n",
        "            else:\n",
        "                if speaker != '':\n",
        "                    if speaker in corp_list:\n",
        "                        df.loc[j] = [speaker, 0, content, date, ticker]\n",
        "                    else:\n",
        "                        df.loc[j] = [speaker, 1, content, date, ticker]\n",
        "                    content = ''\n",
        "                    j += 1\n",
        "\n",
        "                speaker = find_speaker[0]\n",
        "\n",
        "        print(pdf.pages[0].extract_text())\n",
        "        data = data.append(df)\n",
        "\n",
        "    data.to_csv(os.path.join(path, ticker + '.csv'), index=False)\n",
        "    print('converted: ' + os.path.join(path, ticker + '.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result is a file of multiple transcripts - by speaker, whether company personnel or external and date of the earnings call.**"
      ],
      "metadata": {
        "id": "hQrBKn3fNqfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Transcripts_test.csv')\n",
        "data['date'] = pd.Series(pd.to_datetime(data['date'], format='%m-%d-%y'))\n",
        "data.info()"
      ],
      "metadata": {
        "id": "JVnuXIhaNp0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    # removing paragraph numbers\n",
        "    text = re.sub('[0-9]+.\\t', '', str(text))\n",
        "    # removing new line characters\n",
        "    text = re.sub('\\n ', '', str(text))\n",
        "    text = re.sub('\\n', ' ', str(text))\n",
        "    # removing apostrophes\n",
        "    text = re.sub(\"'s\", '', str(text))\n",
        "    # removing hyphens\n",
        "    text = re.sub(\"-\", ' ', str(text))\n",
        "    text = re.sub(\"— \", '', str(text))\n",
        "    # removing quotation marks\n",
        "    text = re.sub('\\\"', '', str(text))\n",
        "    # removing salutations\n",
        "    text = re.sub(\"Mr\\.\", 'Mr', str(text))\n",
        "    text = re.sub(\"Mrs\\.\", 'Mrs', str(text))\n",
        "    # removing any reference to outside text\n",
        "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "IbKh7Gd1MCe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing speeches\n",
        "data['Transcript_clean'] = data['transcript'].apply(clean)"
      ],
      "metadata": {
        "id": "HjW5twTcOeFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Considering a subset of data - namely only company personnel.**"
      ],
      "metadata": {
        "id": "vvZZUAd3OrYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_subset = data[data['qna'] == 0]\n",
        "data_subset = data_subset.drop(['particip'], axis=1)\n",
        "data_subset = data_subset.groupby(['date'])['Transcript_clean'].apply(' '.join).reset_index()\n",
        "print(data_subset.head(10))"
      ],
      "metadata": {
        "id": "naAJzQHnOfNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "WKy49JcsOwtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breaking this down by whole sentences to analyse the contents of each.**"
      ],
      "metadata": {
        "id": "3tu15x6qPg68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences(text):\n",
        "    # split sentences and questions\n",
        "    text = re.split('[.?]', text)\n",
        "    clean_sent = []\n",
        "    for sent in text:\n",
        "        clean_sent.append(sent)\n",
        "    return clean_sent"
      ],
      "metadata": {
        "id": "-bl54mezPS4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences\n",
        "data['sent'] = data['Transcript_clean'].apply(sentences)\n",
        "\n",
        "# create a dataframe containing sentences\n",
        "df2 = pd.DataFrame(columns=['Sent', 'Year', 'Len'])"
      ],
      "metadata": {
        "id": "yYYEYchfPfad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_list = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    for sent in data.loc[i, 'sent']:\n",
        "        wordcount = len(sent.split())\n",
        "        year = data.loc[i, 'Year']\n",
        "\n",
        "        dict1 = {'Year': year, 'Sent': sent, 'Len': wordcount}\n",
        "        row_list.append(dict1)\n",
        "\n",
        "df2 = pd.DataFrame(row_list)\n",
        "print(df2.head(10))"
      ],
      "metadata": {
        "id": "pKWv0OpfPrJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from spacy import displacy\n",
        "# import visualise_spacy_tree\n",
        "from IPython.display import Image, display\n",
        "# load english language model\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])"
      ],
      "metadata": {
        "id": "X4Dv1Pp4Ptsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Information Extraction – Finding Keywords and using Dependencies**\n",
        "SpaCy’s Matcher class. It allows us to match a sequence of words based on certain patterns and using regex to find sentences that only contain keywords"
      ],
      "metadata": {
        "id": "lKO4iT46PyF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prog_sent(text):\n",
        "    patterns = [r'\\b(?i)' + 'coronavirus' + r'\\b',\n",
        "                r'\\b(?i)' + 'Covid-19' + r'\\b',\n",
        "                r'\\b(?i)' + 'lockdown' + r'\\b',\n",
        "                r'\\b(?i)' + 'vaccine' + r'\\b', ]\n",
        "\n",
        "    output = []\n",
        "    flag = 0\n",
        "    for pat in patterns:\n",
        "        if re.search(pat, text) != None:\n",
        "            flag = 1\n",
        "            break\n",
        "    return flag"
      ],
      "metadata": {
        "id": "T_kMloCWPv4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def all_schemes(text, check):\n",
        "    schemes = []\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # initiatives\n",
        "    prog_list = ['coronavirus', 'Covid-19',\n",
        "                 'lockdown', 'vaccine']\n",
        "\n",
        "    # pattern to match initiatives names\n",
        "    pattern = [{'POS': 'DET'},\n",
        "               {'POS': 'PROPN', 'DEP': 'compound'},\n",
        "               {'POS': 'PROPN', 'DEP': 'compound'},\n",
        "               {'POS': 'PROPN', 'OP': '?'},\n",
        "               {'POS': 'PROPN', 'OP': '?'},\n",
        "               {'POS': 'PR OPN', 'OP': '?'},\n",
        "               {'LOWER': {'IN': prog_list}, 'OP': '+'}\n",
        "               ]\n",
        "\n",
        "    if check == 0:\n",
        "        # return blank list\n",
        "        return schemes\n",
        "\n",
        "    # Matcher class object\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"matching\", [pattern])\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for i in range(0, len(matches)):\n",
        "\n",
        "        # match: id, start, end\n",
        "        start, end = matches[i][1], matches[i][2]\n",
        "\n",
        "        if doc[start].pos_ == 'DET':\n",
        "            start = start + 1\n",
        "\n",
        "        # matched string\n",
        "        span = str(doc[start:end])\n",
        "\n",
        "        if (len(schemes) != 0) and (schemes[-1] in span):\n",
        "            schemes[-1] = span\n",
        "        else:\n",
        "            schemes.append(span)\n",
        "\n",
        "    return schemes"
      ],
      "metadata": {
        "id": "24KklpQ7QB5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_subtree(text):\n",
        "    # pattern match for schemes or initiatives\n",
        "    patterns = [r'\\b(?i)' + 'coronavirus' + r'\\b',\n",
        "                r'\\b(?i)' + 'Covid-19' + r'\\b',\n",
        "                r'\\b(?i)' + 'lockdown' + r'\\b',\n",
        "                r'\\b(?i)' + 'vaccine' + r'\\b']\n",
        "\n",
        "    schemes = []\n",
        "    doc = nlp(text)\n",
        "    flag = 0\n",
        "    # if no initiative present in sentence\n",
        "    for pat in patterns:\n",
        "\n",
        "        if re.search(pat, text) != None:\n",
        "            flag = 1\n",
        "            break\n",
        "\n",
        "    if flag == 0:\n",
        "        return schemes\n",
        "\n",
        "    # iterating over sentence tokens\n",
        "    for token in doc:\n",
        "\n",
        "        for pat in patterns:\n",
        "\n",
        "            # if we get a pattern match\n",
        "            if re.search(pat, token.text) != None:\n",
        "\n",
        "                word = ''\n",
        "                # iterating over token subtree\n",
        "                for node in token.subtree:\n",
        "                    # only extract the proper nouns\n",
        "                    if (node.pos_ == 'PROPN'):\n",
        "                        word += node.text + ' '\n",
        "\n",
        "                if len(word) != 0:\n",
        "                    schemes.append(word)\n",
        "\n",
        "    return schemes"
      ],
      "metadata": {
        "id": "XBMgSjweQG_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}